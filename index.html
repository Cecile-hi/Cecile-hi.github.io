
<!doctype html>
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
    <title>Xiao-hui Zhang, BJTU</title>
    <style>
    h1 { padding : 0; margin : 0; }
    body { padding : 20px 0; font-family : Arial; font-size : 16px; background-image : url("img/bg.png"); } 
    #container { width : 900px; margin : 0 auto; background-color : #fff; padding : 50px;  text-align: left; box-shadow: 0px 0px 10px #999;; }
    #me { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0; margin-right:25px;}
    #content { display : block; margin-right : 275px;}
    a { text-decoration : none; }
    a:hover { text-decoration : underline; }
    a:link,a:visited
    {color: #1367a7;}    

    a.invisible { color : inherit; text-decoration : inherit; }
    .publogo { margin-top : 20px; margin-right : 10px; float : left; border : 0; width: 200px; vertical-align: middle;}
    
    .publication { clear : left; padding-bottom : 10px; line-height:22px;}
    .codelogo { margin-right : 10px; float : left; border : 0;}
    .code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
    .code .download a { display : block; margin : 0 15px; float : left;}
    #simpsons { margin : 5px auto; text-align : center; color : #B7B7B7; }

    span.highlight,
    span.highlight a:link,
    span.highlight a:visited{color:#BB2222}

    span.collaborator,
    span.collaborator a:link,
    span.collaborator a:visited{color:#666666}
    </style>

    <script type="text/javascript">
      function showPubs(id) {
        if (id == 0) {
          document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
          document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
          document.getElementById('select1').style = '';
          // document.getElementById('select2').style = '';
        } else if (id == 1) {
          document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_date').innerHTML;
          document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
          document.getElementById('select0').style = '';
          // document.getElementById('select2').style = '';
        } 
        // else {
        //   document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
        //   document.getElementById('select2').style = 'text-decoration:underline;color:#000000';
        //   document.getElementById('select0').style = '';
        //   document.getElementById('select1').style = '';
        // }
      } 
    </script>

</head>
<body>
  <!--#F2F2F2-->
    <center>
    <div id="container">
    <!-- <img src="img/Wei-Chiu-Ma.jpg" align = "right" width = 200> -->
    <img src="img/Personal_Page.png" align = "right" width = 200>
    <div id="content">

    <h1>Xiao-hui Zhang 张晓辉</h1>
    <p>
<!--       Incoming Assistant Professor (Fall 2024)<br>
      Department of Computer Science<br>
      Cornell University<br><br> -->
      Interning @Institute of Automation, Chinese Academy of Science, Beijing, China<br>
      Master Student @ CS/Beijing Jiaotong University, Beijing, China<br>
      Bachelor @ ECE/Beijing Jiaotong University, Beijing, China<br>
      <br>

<!--       Ph.D. Student<br>
       Department of Electrical Engineering and Computer Science<br>
       Massachusetts Institute of Technology<br><br> -->


<!--        Incoming Assistant Professor<br>
       Department of Computer Science<br>
       Cornell University<br><br> -->

       <a href="mailto: 21120320@bjtu.edu.cn">Email</a> / <a href="https://scholar.google.com/citations?user=-7ZwZbMAAAAJ&hl=en">Google scholar</a></p>

    <h2>About Me</h2>   
    <p>I am a final year master student at <a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University(BJTU) and interning at <a href="http://www.ia.cas.cn/">Institute of Automation, Chinese Academy of Science(CASIA)</a>. I interning at <a href="http://www.ia.cas.cn/">Institute of Automation, Chinese Academy of Science(CASIA)</a> where I am advised by <a href="https://www.au.tsinghua.edu.cn/info/1104/2986.html">Jianhua Tao</a>. My research interests mainly lie in the intersection of machine learning and multi-modal learning, with particular interests in continual learning and signal processing tasks such as Deepfake audio detection and multimodal emotion recognition.</p>
    <p>
    <!-- I am an incoming Assistant Professor of Computer Science at <a href="http://cornell.edu">Cornell University</a>, as well as a Young Investigator/Postdoc at <a href="https://allenai.org">AI2</a>/<a href="https://www.washington.edu">University of Washington</a>. -->
    <!-- http://www.ia.cas.cn/ -->

<!--     I am a final year Ph.D. candidate at <a href="http://web.mit.edu">MIT</a> working with <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a> 
    (aka <a href="http://bit.ly/the-great-torralba">the Great Torralba</a>) and <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a>. 
    I am also a part-time Senior Research Scientist at <a href="https://waabi.ai">Waabi</a>.
    My research is supported by a <a href="https://www.siebelscholars.com">Siebel Scholarship</a>. -->

    </p>

    <p>
    My research lies at the intersection of continual learning and multi-modal learning. Specifically, I am interested in developing continuous multimodal tools that allow us to <b>model</b>, <b>reconstruct</b>, and <b>understand</b> the dynamic world from sparse, noisy, and unconstrained sensory data.
<!--     I am interested in computer vision and robotics, especially 3D vision and self-driving vehicles. 
    Over the past few years, I have worked on various topics, such as robot localization, static 3D reconstruction, dynamic motion estimation, large-scale 3D generation, and closed-loop sensor simulation. I examine these tasks not only in controlled settings, but also in sparse, noisy, and sometimes extreme real-world settings in which the models will be deployed. -->
    </p>
    <!--<p> 
    I am a computer vision fanatic. I am interested in a wide array of topics, ranging from low-level vision to high-level vision, and their connections to autonomous systems. Recently I have been thinking more from the 3D perspective.
    </p>-->

    <!--<p>
    I also work closely with <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a> at <a href="https://eng.uber.com/tag/uber-atg-toronto/">Uber ATG Toronto</a> on self-driving problems.</p>-->
    
    <!-- <p>I am a Ph.D. student at <a href="http://web.mit.edu">MIT</a>. I work at <a href="http://csail.mit.edu">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> where I am advised by <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>. My research interests mainly lie in the intersection of computer vision and machine learning. </p>
    -->
    
    <p>
    <!-- I received my Ph.D. from <a href="http://web.mit.edu">MIT</a>, where I worked with <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a> 
    (aka <a href="http://bit.ly/the-great-torralba">the Great Torralba</a>) and <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a>. 
    Previously, I was a Senior Research Scientist at <a href="https://eng.uber.com/tag/uber-atg-toronto/">Uber ATG R&D</a> working on self-driving vehicles. I completed my M.S. in Robotics at <a href="http://www.cs.cmu.edu/">Carnegie Mellon University (CMU)</a>, where I was advised by <a href="http://www.cs.cmu.edu/~kkitani/">Kris M. Kitani</a>.</p>  -->

    <p><span class="highlight"><b>Prospective Advisers:</b></span> I am now looking for full-time Ph.D. positions in the fields of machine learning and multimodal learning. If you have any opportunities or recommendations in these areas, I would greatly appreciate your assistance. Please send me an <a href="mailto: 21120320@bjtu.edu.cn"> email directly.</a>.
    </p>

<!--     <p><span class="highlight"><b>Update:</b></span> I will be joining the <a href="http://www.cs.cornell.edu/">Computer Science Department</a> at <a href="http://cornell.edu">Cornell University</a> as an Assistant Professor in Fall 2024. I am always looking for motivated and talented students! If you are interested in collaborating or joining my group as a PhD/MS/Undergrad student or intern, please read <a href="prospective/html">this</a>.
    </p> -->

    <!--<h3>Pro-bono office hour</h3>
    <p>I have decided to commit 1~2 hours every week to provide guidance, suggestions, and/or mentorships for students from underrepresented groups or whoever is in need. Please fill in this <a href="https://docs.google.com/forms/d/1uYA95BMyFjnvY2-XDbwSAlTz79Ip55cytzrNoROS9ko/edit">form</a>.</p> -->

<!--     <p>I completed my M.S. in Robotics at <a href="http://www.cs.cmu.edu/">Carnegie Mellon University (CMU)</a> where I was advised by <a href="http://www.cs.cmu.edu/~kkitani/">Kris M. Kitani</a>. During my Master's study, I also visited <a href="https://www.utoronto.ca">University of Toronto (UofT)</a> and worked closely with <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a> and <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>. Before coming to the US, I obtained my B.S. in Electrical Engineering at <a href="http://www.ntu.edu.tw">National Taiwan University (NTU)</a>.</p>  -->

  <br>
  <!--<h6>Current/Past Affiliations</h6> -->
  <center>
    <table border="0" cellpadding="0" cellspacing="0" width="900" align="center" bgcolor="#FFFFFF">   
      <tr>
        <td width="20"></td>
            <td width="120" align="center" valign="middle"><img src="img/bjtu.png" height="50" /></td>
            <td width="120" align="center" valign="middle"><img src="img/tsinghua.png" height="80" /></td>            
            <td width="120" align="center" valign="middle"><img src="img/casia.png" height="70" /></td>
            <!-- <td width="120" align="center" valign="middle"><img src="img/logo_cmu_1.png" height="70" /></td>
            <td width="120" align="center" valign="middle"><img src="img/logo_uoft.jpg" height="80" /></td>            
            <td width="120" align="center" valign="middle"><img src="img/logo_ntu.jpg" height="80" /></td> -->
        <td width="20"></td>
      </tr>
<!--       <tr>
        <td width="20"></td>
            <td width="120" align="center" valign="middle">2016 - Present</td>
            <td width="120" align="center" valign="middle">2016 - Present</td>
            <td width="120" align="center" valign="middle">2014 - 2016</td>
            <td width="120" align="center" valign="middle">Summer 2015, 2016</td>
            <td width="120" align="center" valign="middle">2009 - 2013</td>
            <td width="20" align="center" valign="middle"></td>
        <td width="20"></td>
      </tr> -->

  <tr>
  <td colspan = 7>
  <p>&nbsp;</p>
    <h2>Recent News</h2>
    <ul>
<!--         <li><sup><font color="red"><b>NEW</b></font></sup> <b>Seminar</b>: I co-organize the <a href="https://sites.google.com/view/visionseminar">MIT Vision and Graphics Seminar.</a> Please reach out if you are interested in presenting.</li> -->         
        <li><sup><font color="red"><b>NEW</b></font></sup> <b>Intern</b>: I an currently interning remotely at the University of North Carolina at Chapel Hill(UNC), USA under Prof. <a href='https://www.huaxiuyao.io/'>Huaxiu Yao and Prof. <a href='https://cs.unc.edu/person/mohit-bansal/'>Mohit Bansal about multimodal learning, our work has been submitted to <a href='https://iclr.cc/'>ICLR 2024</li>
        <br>
        <li><sup><font color="red"><b>NEW</b></font></sup> <b>August. 2023</b>: Our work on continual learning & Deepfake audio detection has been submitted to <a href='https://aaai.org/aaai-conference/'> AAAI 2024</li>

        <li><sup><font color="red"><b>NEW</b></font></sup> <b>April. 2023</b>: Our work on continual learning & Deepfake audio detection has been accepted by <a href='https://icml.cc/'> ICML 2023. The paper and code has already been released in <a href='https://arxiv.org/abs/2308.03300'>arxiv and <a href='https://github.com/Cecile-hi/Regularized-Adaptive-Weight-Modification'>github. Check it out!</li>           
        <!-- <li><sup><font color="red"><b>NEW</b></font></sup> <b>Mar. 2023</b>: Check out our latest effort on closed-loop sensor simulation, LiDAR generation, and thermal imaging!</li>                -->
        <li><b>June. 2023</b>: Three papers accepted to IJCAI 2023 DADA Workshop. Check out our work on Deepfake Audio Detection & LoRA <a href='https://arxiv.org/pdf/2306.05617.pdf'>arxiv</li>      
        <li> <b>August. 2023</b>: One paper accepted to CICAI 2023. Check out our work on Time-Sparse Transducer for Speech Recognition <a href='https://arxiv.org/pdf/2307.08323.pdf'>arxiv</li>      
        <li><b>June. 2023</b>: One paper on Deepfake Audio Detection has been submitted to TPAMI</li>
        <li><b>June. 2021</b>: Our work (A multilingual framework based on pre-training model for speech emotion recognition) is accepted to APSIPA 2021! Check it out! <a href='http://www.apsipa.org/proceedings/2021/pdfs/0000750.pdf'>arxiv</li>
        <!--<li><b>Sep. 2016</b>: I joined the EECS department at MIT as a Ph.D. student!</li>
        <li><b>May. - Aug. 2016</b>: I visited University of Toronto again this summer!</li>-->
       <li><b>June. 2021</b>: I graduated from BJTU --- thanks Mangui Liang for all your support in the past four years!</li>        
               
    </ul>
    </td>
    </tr>

  <tr>
  <td colspan = 7>
  <p>&nbsp;</p>
    <h2 style="display:inline;">Research Projects</h2>
      &nbsp      
      (<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
       <a href="" id="select1" onclick="showPubs(1); return false;">show by date</a>) <!-- /
       <a href="" id="select2" onclick="showPubs(2); return false;">show by topic</a>) -->        
    <!--<div style="top:-10px; position:relative;">-->
  <table border="0" cellpadding="0" cellspacing="0" width="900" align="center" bgcolor="#FFFFFF" id="pubs"></table>

  <script id="pubs_selected" language="text">
  <!-- UniSim -->
  <tr>
    <td>
      <img border=0 src="img/unisim.mp4" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>UniSim: A Neural Closed-Loop Sensor Simulator</strong>
      <br>
      <span class="collaborator">Ze Yang*, </span>
      <span class="collaborator">Yun Chen*, </span>
      <span class="collaborator">Jingkang Wang*, </span>
      <span class="collaborator">Sivabalan Manivasagam*, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Joyce Anqi Yang, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CVPR 2023</b></a> /
      <a href="" target="">project page (coming soon)</a> /
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf" target="">paper</a> / 
      <a href="http://www.cs.toronto.edu/~wangjk/publications/unisim/unisim_final_v2_4k.mp4" target="">4K demo</a> /
      <a href="https://waabi.ai/wp-content/uploads/2023/05/UniSim-video_compressed.mp4" target="">video (8 mins)</a>
      <br>
      <span class="highlight"><b>Highlight presentation</b></span>    
      </div>
    </td>
  </tr> 

  <!-- UltraLidar -->
  <tr>
    <td>
      <img border=0 src="img/lidar-gen.mp4" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation</strong>
      <br>
      <span class="collaborator">Yuwen Xiong, </span>      
      Wei-Chiu Ma,       
      <span class="collaborator">Jingkang Wang, </span>      
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CVPR 2023</b></a> /
      <a href="" target="">project page (coming soon)</a> /
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2023_paper.pdf" target="">paper</a> / 
      <a href="https://waabi.ai/wp-content/uploads/2023/05/UltraLidar-video.mov" target="">video (1 min)</a>
      </div>
    </td>
  </tr> 

  <!-- SGAM -->
  <tr>
    <td>
      <img border=0 src="papers/neurips22-sgam/sgam.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>SGAM: Building a Virtual 3D World through Simultaneous Generation and Mapping</strong>
      <br>
      <span class="collaborator">Yuan Shen, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang</span>
      <br>
      <a href="" target=""><b>NeurIPS 2022</b></a> /
      <a href="https://yshen47.github.io/sgam/" target="">project page</a> /
      <a href="https://openreview.net/pdf?id=17KCLTbRymw" target="">paper</a> /
      <a href="https://github.com/yshen47/SGAM" target="">code</a>
      </div>
    </td>
  </tr> 

  <!-- CADSim -->
  <tr>
    <td>
      <img border=0 src="papers/corl22-cad-sim/cad-sim.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Realistic and Controllable Sensor Simulation</strong>
      <br>
      <span class="collaborator">Jingkang Wang, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Yun Chen, </span>
      <span class="collaborator">Ze Yang, </span>
      <span class="collaborator">Ioan Andrei Bârsan, </span>
      <span class="collaborator">Joyce Anqi Yang, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CoRL 2022</b></a> /
      <a href="http://www.cs.toronto.edu/~wangjk/publications/cadsim.html" target="">project page</a> /
      <a href="https://openreview.net/pdf?id=Mp3Y5jd7rnW" target="">paper</a> /
      <a href="http://www.cs.toronto.edu/~wangjk/publications/cadsim/cadsim_corl_denoise.mp4" target="">video</a>
      </div>
    </td>
  </tr> 

  <!-- VC -->
  <tr>
    <td>
      <img border=0 src="img/vc-3.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Virtual Correspondence: Humans as a Cue for Extreme-View Geometry</strong>
      <br>
      Wei-Chiu Ma, 
      <span class="collaborator">Anqi Joyce Yang, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Raquel Urtasun, </span>
      <span class="collaborator">Antonio Torralba</span>
      <br>
      <a href="" target="_new"><b>CVPR 2022</b></a> /
      <a href="virtual-correspondence" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2206.08365.pdf" target="_new">paper</a> /
      <a href="https://virtual-correspondence.github.io/img/vc_demo.mp4" target="_new">video (1.5 mins)</a> /
      <a href="https://youtu.be/W9odd2F2Bx4" target="_new">video (5 mins)</a> /
      <a href="https://news.mit.edu/2022/seeing-whole-from-some-parts-0617" target="_new">MIT News</a> / 
      <a href="https://techxplore.com/news/2022-06-vision-technique-3d-2d-images.html" target="_new">TechXplore</a>
      </div>
    </td>
  </tr> 

  <!-- NeurMiPs -->
  <tr>
    <td>
      <img border=0 src="img/neurmips.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>NeurMiPs: Neural Mixture of Planar Experts for View Synthesis</strong>
      <br>
      <span class="collaborator">Zhi-Hao Lin, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Hao-Yu Max Hsu, </span>
      <span class="collaborator">Yu-Chiang Frank Wang, </span>
      <span class="collaborator">Shenlong Wang</span>
      <br>
      <a href="" target="_new"><b>CVPR 2022</b></a> /
      <a href="https://zhihao-lin.github.io/neurmips/" target="_new">project page</a> / 
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_NeurMiPs_Neural_Mixture_of_Planar_Experts_for_View_Synthesis_CVPR_2022_paper.pdf" target="_new">paper</a> / 
      <a href="https://github.com/zhihao-lin/neurmips" target="_new">code</a> / 
      <a href="https://www.youtube.com/watch?v=PV1dCTWL5Oo" target="_new">video</a>
      </div>
    </td>
  </tr>  

  <!-- BARF -->
  <tr>
    <td>
      <img border=0 src="img/barf.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>BARF: Bundle-Adjusting Neural Radiance Fields </strong>
      <br>
      <span class="collaborator">Chen-Hsuan Lin, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Antonio Torralba, </span>
      <span class="collaborator">Simon Lucey</span>
      <br>
      <a href="" target="_new"><b>ICCV 2021</b></a> /
      <a href="https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2104.06405.pdf" target="_new">arXiv</a> / 
      <a href="https://github.com/chenhsuanlin/bundle-adjusting-NeRF" target="_new">code</a> / 
      <a href="https://www.youtube.com/watch?v=dCmCZs2Hpi0" target="_new">video</a> / 
      <a href="https://read.deeplearning.ai/the-batch/issue-95#3d-scene-synthesis-for-the-real-world" target="_new">news coverage (The Batch: DeepLearning.AI)</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>
      </div>
    </td>
  </tr>    

  <!-- feedback solver -->
  <tr>
    <td>
      <img border=0 src="img/deep-optimizer-teaser.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Deep Feedback Inverse Problem Solver</strong>
      <br>
      Wei-Chiu Ma,       
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Jiayuan Gu, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Antonio Torralba, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <!-- <br><span class="highlight"><b>CVPR 2020</b></span> -->
      <br>
      <a href="" target="_new"><b>ECCV 2020</b></a> / 
      <a href="papers/eccv20-deep-optimizer/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2101.07719.pdf" target="_new">arXiv</a> / 
      <a href="papers/eccv20-deep-optimizer/img/deep-feedback-inverse-problem-solver-short.mp4" target="_new">short video (1.5 mins)</a> / 
      <a href="papers/eccv20-deep-optimizer/img/deep-feedback-inverse-problem-solver-long.mp4" target="_new">long video (10 mins)</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>
      </div>
    </td>
  </tr>  

  <!-- LiDARSim -->
  <tr>
    <td>
      <img border=0 src="img/lidarsim.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>LidarSIM: Realistic LiDAR Simulation by Leveraging the Real World</strong>
      <br>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Kelvin Wong, Wenyuan Zeng, </span>
      <span class="collaborator">Bin Yang, </span>
      <span class="collaborator">Shuhan Tan, </span>
      <span class="collaborator">Mikita Sazanovich, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Raquel Urtasun</span>
      <!-- <br><span class="highlight"><b>CVPR 2020</b></span> -->
      <br>
      <a href="" target="_new"><b>CVPR 2020</b></a> / 
      <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.pdf" target="_new">paper</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>
      </div>
    </td>
  </tr>  

  <!-- sparse loc -->
  <tr>
    <td>
      <a href="https://arxiv.org/pdf/1908.03274.pdf"><img border=0 src="img/light-loc.gif" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization</strong>
      <br>
      Wei-Chiu Ma*, 
      <span class="collaborator">Ignacio Tartavull*, </span>
      <span class="collaborator">Ioan Andrei Bârsan*, </span>
        <span class="collaborator">Shenlong Wang*, </span>
        <span class="collaborator">Min Bai, </span>
        <span class="collaborator">Gellert Mattyus, </span>
        <span class="collaborator">Namdar Homayounfar, </span>
        <span class="collaborator">Shrinidhi K. Lakshmikanth, </span>
        <span class="collaborator">Andrei Pokrovsky, </span>
        <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>IROS 2019</b></a> / 
      <a href="https://arxiv.org/pdf/1908.03274.pdf" target="_new">arXiv</a> / 
      <a href="https://www.youtube.com/watch?v=-_PvPPr7y28" target="_new">video</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>      
      </div>
    </td>
  </tr>

  <!-- DRISF -->
  <tr>
    <td>
      <a href="papers/cvpr19-drisf/"><img border=0 src="img/cvpr-drisf.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Deep Rigid Instance Scene Flow</strong>
      <br>
      Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Rui Hu, </span>
      <span class="collaborator">Yuwen Xiong, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2019</b></a> / 
      <a href="papers/cvpr19-drisf/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/1904.08913.pdf" target="_new">arXiv</a> / 
      <a href="papers/cvpr19-drisf/paper.pdf" target="_new">paper + supp (uncompressed)</a> / 
      <a href="papers/cvpr19-drisf/gn_iteration.gif" target="_new">GN solver gif</a>
      <br>
      <br>
      Deep structured scene flow model that <span class="highlight"><b>rank 1st</b></span> on <a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php">KITTI Scene Flow Benchmark</a>.
      <br>Faster than prior art by <span class="highlight"><b>800 times</b></span>.
      </div>
    </td>
  </tr>

  <!-- Deep Pruner -->
  <tr>
    <td>
      <a href="papers/iccv19-deep-pruner/deeppruner.pdf"><img border=0 src="img/deeppruner.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch</strong>
      <br>
      <span class="collaborator">Shivam Duggal, </span>
      <span class="collaborator">Shenlong Wang, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Rui Hu, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>ICCV 2019</b></a> / 
      <a href="https://arxiv.org/pdf/1909.05845.pdf" target="_new">arXiv</a> / 
      <a href="https://github.com/uber-research/DeepPruner/" target="_new">code</a> / 
      <a href="https://github.com/uber-research/DeepPruner/tree/master/DifferentiablePatchMatch" target="_new"> differentiable PatchMatch module</a>
      <br>
      <br><span class="highlight"><b>Real-time</b></span> stereo estimation (62 ms) via <span class="highlight"><b>Differentiable PatchMatch</b></span>! 
      </div>
    </td>
  </tr>   

  <!-- Continuous Conv -->
  <tr>
    <td>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf"><img border=0 src="img/cont_conv.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Deep Parametric Continuous Convolutional Neural Networks</strong>
      <br>
      <span class="collaborator">Shenlong Wang*, </span>
      <span class="collaborator">Simon Suo*, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Andrei Pokrovsky, </span>
      <span class="collaborator">and Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2018</b></a> / 
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf" target="_new">paper</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>           
      </div>
    </td>
  </tr>  

  <!-- Sun CNN -->
  <tr>
    <td>
      <a href="https://youtu.be/tOVWptik0qI" target="_blank"><img border=0 src="img/lost2.gif" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Find Your Way by Observing the Sun and Other Semantic Cues</strong>
      <br>Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Marcus A. Brubaker, </span>
      <span class="collaborator">Sanja Fidler, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>ICRA 2017</b></a> / 
      <a href="http://arxiv.org/pdf/1606.07415.pdf" target="_new">arXiv</a> / 
      <a href="https://youtu.be/tOVWptik0qI" target="_blank">demo video</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>        
      </div>
    </td>
  </tr>

  </script>


  <script id="pubs_by_date" language="text">
  <tr>
    <td>
      <img border=0 src="img/unisim.mp4" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>UniSim: A Neural Closed-Loop Sensor Simulator</strong>
      <br>
      <span class="collaborator">Ze Yang*, </span>
      <span class="collaborator">Yun Chen*, </span>
      <span class="collaborator">Jingkang Wang*, </span>
      <span class="collaborator">Sivabalan Manivasagam*, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Joyce Anqi Yang, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CVPR 2023</b></a> /
      <a href="" target="">project page (coming soon)</a> /
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf" target="">paper</a> / 
      <a href="http://www.cs.toronto.edu/~wangjk/publications/unisim/unisim_final_v2_4k.mp4" target="">4K demo</a> /
      <a href="https://waabi.ai/wp-content/uploads/2023/05/UniSim-video_compressed.mp4" target="">video (8 mins)</a>
      <br>
      <span class="highlight"><b>Highlight presentation</b></span>      
      </div>
    </td>
  </tr> 

  <tr>
    <td>
      <img border=0 src="img/lidar-gen.mp4" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation</strong>
      <br>
      <span class="collaborator">Yuwen Xiong, </span>      
      Wei-Chiu Ma,       
      <span class="collaborator">Jingkang Wang, </span>      
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CVPR 2023</b></a> /
      <a href="" target="">project page (coming soon)</a> /
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2023_paper.pdf" target="">paper</a> / 
      <a href="https://waabi.ai/wp-content/uploads/2023/05/UltraLidar-video.mov" target="">video (1 min)</a>
      </div>
    </td>
  </tr> 

  <tr>
    <td>
      <img border=0 src="img/thermal-im.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging</strong>
      <br>
      <span class="collaborator">Zitian Tang*, </span>
      <span class="collaborator">Wenjie Yeh*, </span>      
      Wei-Chiu Ma,       
      <span class="collaborator">Hang Zhao</span>
      <br>
      <a href="" target=""><b>CVPR 2023</b></a> /
      <a href="https://arxiv.org/pdf/2304.13651.pdf" target="">arXiv</a> / 
      <a href="https://github.com/ZitianTang/Thermal-IM" target="">dataset</a>      
      </div>
    </td>
  </tr> 


  <tr>
    <td>
      <img border=0 src="papers/neurips22-sgam/sgam.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>SGAM: Building a Virtual 3D World through Simultaneous Generation and Mapping</strong>
      <br>
      <span class="collaborator">Yuan Shen, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang</span>
      <br>
      <a href="" target=""><b>NeurIPS 2022</b></a> /
      <a href="https://yshen47.github.io/sgam/" target="">project page</a> /
      <a href="https://openreview.net/pdf?id=17KCLTbRymw" target="">paper</a> /
      <a href="https://github.com/yshen47/SGAM" target="">code</a>
      </div>
    </td>
  </tr> 

  <tr>
    <td>
      <img border=0 src="papers/corl22-cad-sim/cad-sim.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Realistic and Controllable Sensor Simulation</strong>
      <br>
      <span class="collaborator">Jingkang Wang, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Yun Chen, </span>
      <span class="collaborator">Ze Yang, </span>
      <span class="collaborator">Ioan Andrei Bârsan, </span>
      <span class="collaborator">Joyce Anqi Yang, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target=""><b>CoRL 2022</b></a> /
      <a href="http://www.cs.toronto.edu/~wangjk/publications/cadsim.html" target="">project page</a> /
      <a href="https://openreview.net/pdf?id=Mp3Y5jd7rnW" target="">paper</a> /
      <a href="http://www.cs.toronto.edu/~wangjk/publications/cadsim/cadsim_corl_denoise.mp4" target="">video</a>
      </div>
    </td>
  </tr> 

  <tr>
    <td>
      <img border=0 src="papers/corl22-mira/mira.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>MIRA: Mental Imagery for Robotic Affordances</strong>
      <br>
      <span class="collaborator">Lin Yen-Chen, </span>
      <span class="collaborator">Pete Florence, </span>
      <span class="collaborator">Andy Zeng, </span>
      <span class="collaborator">Jonathan T. Barron, </span>
      <span class="collaborator">Yilun Du, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Anthony Simeonov, </span>
      <span class="collaborator">Alberto Rodriguez Garcia, </span>
      <span class="collaborator">Phillip Isola</span>
      <br>
      <a href="" target=""><b>CoRL 2022</b></a> /
      <a href="http://yenchenlin.me/mira/" target="">project page</a> /
      <a href="https://arxiv.org/pdf/2212.06088.pdf" target="">paper</a> /
      <a href="https://www.youtube.com/watch?v=3GH-s9db5e0&ab_channel=MIRA" target="">video</a> 
      </div>
    </td>
  </tr> 


  <tr>
    <td>
      <img border=0 src="img/vc-3.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Virtual Correspondence: Humans as a Cue for Extreme-View Geometry</strong>
      <br>
      Wei-Chiu Ma, 
      <span class="collaborator">Anqi Joyce Yang, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Raquel Urtasun, </span>
      <span class="collaborator">Antonio Torralba</span>
      <br>
      <a href="" target="_new"><b>CVPR 2022</b></a> /
      <a href="virtual-correspondence" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2206.08365.pdf" target="_new">paper</a> /
      <a href="https://virtual-correspondence.github.io/img/vc_demo.mp4" target="_new">video (1.5 mins)</a> /
      <a href="https://youtu.be/W9odd2F2Bx4" target="_new">video (5 mins)</a> /
      <a href="https://news.mit.edu/2022/seeing-whole-from-some-parts-0617" target="_new">MIT News</a> / 
      <a href="https://techxplore.com/news/2022-06-vision-technique-3d-2d-images.html" target="_new">TechXplore</a>
      </div>
    </td>
  </tr> 


  <tr>
    <td>
      <img border=0 src="img/neurmips.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>NeurMiPs: Neural Mixture of Planar Experts for View Synthesis</strong>
      <br>
      <span class="collaborator">Zhi-Hao Lin, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Hao-Yu Max Hsu, </span>
      <span class="collaborator">Yu-Chiang Frank Wang, </span>
      <span class="collaborator">Shenlong Wang</span>
      <br>
      <a href="" target="_new"><b>CVPR 2022</b></a> /
      <a href="https://zhihao-lin.github.io/neurmips/" target="_new">project page</a> / 
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_NeurMiPs_Neural_Mixture_of_Planar_Experts_for_View_Synthesis_CVPR_2022_paper.pdf" target="_new">paper</a> / 
      <a href="https://github.com/zhihao-lin/neurmips" target="_new">code</a> / 
      <a href="https://www.youtube.com/watch?v=PV1dCTWL5Oo" target="_new">video</a>
      </div>
    </td>
  </tr>  


  <tr>
    <td>
      <img border=0 src="img/sdf_secrets.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Mending Neural Implicit Modeling for 3D Vehicle Reconstruction in the Wild</strong>
      <br>
      <span class="collaborator">Shivam Duggal*, </span>
      <span class="collaborator">Zihao Wang*, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Justin Liang, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target="_new"><b>WACV 2022</b></a> /
      <a href="https://arxiv.org/pdf/2101.06860.pdf" target="_new">arXiv</a> / 
      <a href="https://www.youtube.com/watch?v=IRygme5J-Ng" target="_new">video</a>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <img border=0 src="img/barf.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>BARF: Bundle-Adjusting Neural Radiance Fields </strong>
      <br>
      <span class="collaborator">Chen-Hsuan Lin, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Antonio Torralba, </span>
      <span class="collaborator">Simon Lucey</span>
      <br>
      <a href="" target="_new"><b>ICCV 2021</b></a> /
      <a href="https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2104.06405.pdf" target="_new">arXiv</a> / 
      <a href="https://github.com/chenhsuanlin/bundle-adjusting-NeRF" target="_new">code</a> / 
      <a href="https://www.youtube.com/watch?v=dCmCZs2Hpi0" target="_new">video</a> / 
      <a href="https://read.deeplearning.ai/the-batch/issue-95#3d-scene-synthesis-for-the-real-world" target="_new">news coverage (The Batch: DeepLearning.AI)</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>
      </div>
    </td>
  </tr>  


  <tr>
    <td>
      <img border=0 src="img/s3-skinning.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling </strong>
      <br>
      <span class="collaborator">Ze Yang, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Zeng Huang, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Xinchen Yan, </span>
      <span class="collaborator">Ersin Yumer, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target="_new"><b>CVPR 2021</b></a> / 
      <a href="https://arxiv.org/pdf/2101.06571.pdf" target="_new">arXiv</a> / 
      <a href="https://www.youtube.com/watch?v=oCpFJZrVDCs" target="_new">video (5 mins)</a>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <img border=0 src="img/lime.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Recovering and Simulating Pedestrians in the Wild </strong>
      <br>
      <span class="collaborator">Ze Yang, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Ming Liang, </span>
      <span class="collaborator">Bin Yang, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target="_new"><b>CoRL 2020</b></a> / 
      <a href="https://arxiv.org/abs/2011.08106" target="_new">arXiv</a> / 
      <a href="https://www.youtube.com/watch?v=iBsIoCJ9I1s" target="_new">video</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <img border=0 src="img/deep-optimizer-teaser.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Deep Feedback Inverse Problem Solver</strong>
      <br>
      Wei-Chiu Ma,       
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Jiayuan Gu, </span>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Antonio Torralba, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <!-- <br><span class="highlight"><b>CVPR 2020</b></span> -->
      <br>
      <a href="" target="_new"><b>ECCV 2020</b></a> / 
      <a href="papers/eccv20-deep-optimizer/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/2101.07719.pdf" target="_new">arXiv</a> / 
      <a href="papers/eccv20-deep-optimizer/img/deep-feedback-inverse-problem-solver-short.mp4" target="_new">short video (1.5 mins)</a> / 
      <a href="papers/eccv20-deep-optimizer/img/deep-feedback-inverse-problem-solver-long.mp4" target="_new">long video (10 mins)</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <img border=0 src="img/shape-comp.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Weakly-supervised 3D Shape Completion in the Wild</strong>
      <br>
      <span class="collaborator">Jiayuan Gu, </span>      
      Wei-Chiu Ma,       
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Wenyuan Zeng, </span>
      <span class="collaborator">Zihao Wang, </span>
      <span class="collaborator">Yuwen Xiong, </span>
      <span class="collaborator">Hao Su, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <!-- <br><span class="highlight"><b>CVPR 2020</b></span> -->
      <br>
      <a href="" target="_new"><b>ECCV 2020</b></a> / 
      <a href="https://arxiv.org/pdf/2008.09110.pdf" target="_new">arXiv</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>
      </div>
    </td>
  </tr> 

  <tr>
    <td>
      <img border=0 src="img/levelset-rcnn.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>LevelSet R-CNN: A Deep Variational Method for Instance Segmentation</strong>
      <br>
      <span class="collaborator">Namdar Homayounfar*, </span>
      <span class="collaborator">Yuwen Xiong*, </span>
      <span class="collaborator">Justin Liang*, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target="_new"><b>ECCV 2020</b></a> / 
      <a href="https://arxiv.org/pdf/2007.15629.pdf" target="_new">arXiv</a>
      </div>
    </td>
  </tr>   

  <tr>
    <td>
      <img border=0 src="img/video-compression.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>Conditional Entropy Coding for Efficient Video Compression</strong>
      <br>
      <span class="collaborator">Jerry Junkai Liu, </span>      
      <span class="collaborator">Shenlong Wang, </span>
      Wei-Chiu Ma,       
      <span class="collaborator">Meet Shah, </span>
      <span class="collaborator">Rui Hu, </span>
      <span class="collaborator">Pranaab Dhawan, </span>
      <span class="collaborator">Raquel Urtasun</span>
      <br>
      <a href="" target="_new"><b>ECCV 2020</b></a> / 
      <a href="https://arxiv.org/pdf/2008.09180.pdf" target="_new">arXiv</a>
      </div>
    </td>
  </tr>   


  <tr>
    <td>
      <a href="https://arxiv.org/pdf/1912.02801.pdf"><img border=0 src="img/polytransform-new" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>PolyTransform: Deep Polygon Transformer for Instance Segmentation </strong>
      <br>
      <span class="collaborator">Justin Liang, </span>
      <span class="collaborator">Namdar Homayounfar, </span>
      Wei-Chiu Ma,
      <span class="collaborator">Yuwen Xiong, </span> 
      <span class="collaborator">Rui Hu, </span> 
      <span class="collaborator">Raquel Urtasun </span>
      <br><a href="" target="_new"><b>CVPR 2020</b></a> / 
      <a href="https://arxiv.org/pdf/1912.02801.pdf" target="_new">arXiv</a> / 
      <a href="http://justin-liang.com/papers/Liang_PolyTransform_Supp.pdf" target="_new">supp</a> / 
      <a href="http://justin-liang.com/papers/Liang_PolyTransform_Video.zip" target="_new">video</a> / 
      <a href="" target="_new">code (coming soon!)</a>
      <br>
      <br>State-of-the-art performance on Cityscapes! Consistent improvements across all backbones!
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <img border=0 src="img/lidarsim.gif" class="publogo">
    </td>
    <td>
      <div class="publication">
      <br>
      <p><strong>LidarSIM: Realistic LiDAR Simulation by Leveraging the Real World</strong>
      <br>
      <span class="collaborator">Sivabalan Manivasagam, </span>
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Kelvin Wong, Wenyuan Zeng, </span>
      <span class="collaborator">Bin Yang, </span>
      <span class="collaborator">Shuhan Tan, </span>
      <span class="collaborator">Mikita Sazanovich, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Raquel Urtasun</span>
      <!-- <br><span class="highlight"><b>CVPR 2020</b></span> -->
      <br>
      <a href="" target="_new"><b>CVPR 2020</b></a> / 
      <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.pdf" target="_new">paper</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <a href="https://arxiv.org/pdf/1908.03274.pdf"><img border=0 src="img/light-loc.gif" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization</strong>
      <br>
      Wei-Chiu Ma*, 
      <span class="collaborator">Ignacio Tartavull*, </span>
      <span class="collaborator">Ioan Andrei Bârsan*, </span>
        <span class="collaborator">Shenlong Wang*, </span>
        <span class="collaborator">Min Bai, </span>
        <span class="collaborator">Gellert Mattyus, </span>
        <span class="collaborator">Namdar Homayounfar, </span>
        <span class="collaborator">Shrinidhi K. Lakshmikanth, </span>
        <span class="collaborator">Andrei Pokrovsky, </span>
        <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>IROS 2019</b></a> / 
      <a href="https://arxiv.org/pdf/1908.03274.pdf" target="_new">arXiv</a> / 
      <a href="https://www.youtube.com/watch?v=-_PvPPr7y28" target="_new">video</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>      
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <a href="papers/cvpr19-drisf/"><img border=0 src="img/cvpr-drisf.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Deep Rigid Instance Scene Flow</strong>
      <br>
      Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Rui Hu, </span>
      <span class="collaborator">Yuwen Xiong, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2019</b></a> / 
      <a href="papers/cvpr19-drisf/" target="_new">project page</a> / 
      <a href="https://arxiv.org/pdf/1904.08913.pdf" target="_new">arXiv</a> / 
      <a href="papers/cvpr19-drisf/paper.pdf" target="_new">paper + supp (uncompressed)</a> / 
      <a href="papers/cvpr19-drisf/gn_iteration.gif" target="_new">GN solver gif</a>
      <br>
      <br>
      Deep structured scene flow model that <span class="highlight"><b>rank 1st</b></span> on <a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php">KITTI Scene Flow Benchmark</a>.
      <br>Faster than prior art by <span class="highlight"><b>800 times</b></span>.
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <a href=""><img border=0 src="img/cvpr-road-boundary.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Convolutional Recurrent Network for Road Boundary Extraction</strong>
      <br>
      <span class="collaborator">Justin Liang*, </span>
      <span class="collaborator">Namdar Homayounfar*, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2019</b></a> / 
      <a href="https://nhoma.github.io/papers/road_cvpr19.pdf" target="_new">paper</a> / 
      <a href="https://nhoma.github.io/papers/road_cvpr19_supp.pdf" target="_new">supp</a>
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <a href="papers/iccv19-deep-pruner/deeppruner.pdf"><img border=0 src="img/deeppruner.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch</strong>
      <br>
      <span class="collaborator">Shivam Duggal, </span>
      <span class="collaborator">Shenlong Wang, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Rui Hu, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>ICCV 2019</b></a> / 
      <a href="https://arxiv.org/pdf/1909.05845.pdf" target="_new">arXiv</a> / 
      <a href="https://github.com/uber-research/DeepPruner/" target="_new">code</a> / 
      <a href="https://github.com/uber-research/DeepPruner/tree/master/DifferentiablePatchMatch" target="_new"> differentiable PatchMatch module</a>
      <br>
      <br><span class="highlight"><b>Real-time</b></span> stereo estimation (62 ms) via <span class="highlight"><b>Differentiable PatchMatch</b></span>! 
      </div>
    </td>
  </tr>  

  <tr>
    <td>
      <a href="http://www.mit.edu/~hangzhao/videos/SoM_supp.mp4"><img border=0 src="img/sound_of_motions" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>The Sound of Motions</strong>
      <br>
      <span class="collaborator">Hang Zhao, </span>
      <span class="collaborator">Chuang Gan, </span>
      Wei-Chiu Ma,  
      <span class="collaborator">Antonio Torralba </span>
      <br>
      <a href="" target="_new"><b>ICCV 2019</b></a> / 
      <a href="https://arxiv.org/pdf/1904.05979.pdf" target="_new">arXiv</a> / 
      <a href="http://www.mit.edu/~hangzhao/videos/SoM_supp.mp4" target="_new">demon video</a>
      </div>
    </td>
  </tr>    

  <tr>
    <td>
      <a href=""><img border=0 src="img/iccv19-dag.jpg" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>DAG-Mapper: Learning to Map by Discovering Lane Topology</strong>
      <br>
      <span class="collaborator">Namdar Homayounfar, </span>
      Wei-Chiu Ma,  
      <span class="collaborator">Justin Liang, </span>
      <span class="collaborator">Xinyu Wu, </span>
      <span class="collaborator">Jack Fan, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>ICCV 2019</b></a> / 
      <a href="https://nhoma.github.io/papers/dagmapper_iccv19.pdf" target="_new">paper</a> / 
      <a href="https://nhoma.github.io/papers/dagmapper_iccv19_supp.pdf" target="_new">supp</a>
      </div>
    </td>
  </tr>  
 
  <tr>
    <td>
      <a href="papers/eccv18-intrinsics/top.pdf"><img border=0 src="img/cover_intrinsic.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Single Image Intrinsic Decomposition without a Single Intrinsic Image</strong>
      <br>
      Wei-Chiu Ma,  
      <span class="collaborator">Hang Chu, </span>
      <span class="collaborator">Bolei Zhou, </span>
      <span class="collaborator">Raquel Urtasun, </span>
      <span class="collaborator">Antonio Torralba </span>
      <br>
      <a href="" target="_new"><b>ECCV 2018</b></a> / 
      <a href="papers/eccv18-intrinsics/top.pdf" target="_new">paper</a>
      </div>
    </td>
  </tr>

   <tr>
    <td>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf"><img border=0 src="img/cont_conv.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Deep Parametric Continuous Convolutional Neural Networks</strong>
      <br>
      <span class="collaborator">Shenlong Wang*, </span>
      <span class="collaborator">Simon Suo*, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Andrei Pokrovsky, </span>
      <span class="collaborator">and Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2018</b></a> / 
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf" target="_new">paper</a>
      <br>
      <span class="highlight"><b>Spotlight presentation</b></span>           
      </div>
    </td>
  </tr>  
  
  <tr>
    <td>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.pdf"><img border=0 src="img/surf_conv.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>SurfConv: Bridging 3D and 2D Convolution for RGBD Images</strong>
      <br>
      <span class="collaborator">Hang Chu, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Kaustav Kundu, </span>
      <span class="collaborator">Raquel Urtasun, </span>
      <span class="collaborator">and Sanja Fidler </span>
      <br>
      <a href="" target="_new"><b>CVPR 2018</b></a> / 
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.pdf" target="_new">paper</a> / 
      <a href="https://github.com/chuhang/SurfConv" target="_new">code</a>

      </div>
    </td>
  </tr>  
  
  <tr>
    <td>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf"><img border=0 src="img/snake.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Hierarchical Recurrent Attention Networks for Structured Online Maps</strong>
      <br>
      <span class="collaborator">Namdar Homayounfar, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Shrinidhi K. Lakshmikanth, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>CVPR 2018</b></a> / 
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf" target="_new">paper</a> / 
      <a href="https://nhoma.github.io/papers/hran_cvpr18_supp.pdf" target="_new">supp</a>
      </div>
    </td>
  </tr>  
  
  <tr>
    <td>
      <img border=0 src="img/book1.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <p><strong>Activity Forecasting: An Invitation to Predictive Perception</strong>
      <br>
      <span class="collaborator">Kris M. Kitani, </span>
      <span class="collaborator">De-An Huang, </span>
      Wei-Chiu Ma
      <br>
      <a href="" target="_new"><b>Group and Crowd Behavior for Computer Vision. Chapter 12, 2017</b></a> / 
      <a href="http://www.sciencedirect.com/science/article/pii/B978012809276700014X" target="_new">link</a>
      </div>
    </td>
  </tr>  
  
  <tr>
    <td>
      <a href="https://youtu.be/tOVWptik0qI" target="_blank"><img border=0 src="img/lost2.gif" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Find Your Way by Observing the Sun and Other Semantic Cues</strong>
      <br>Wei-Chiu Ma, 
      <span class="collaborator">Shenlong Wang, </span>
      <span class="collaborator">Marcus A. Brubaker, </span>
      <span class="collaborator">Sanja Fidler, </span>
      <span class="collaborator">Raquel Urtasun </span>
      <br>
      <a href="" target="_new"><b>ICRA 2017</b></a> / 
      <a href="http://arxiv.org/pdf/1606.07415.pdf" target="_new">arXiv</a> / 
      <a href="https://youtu.be/tOVWptik0qI" target="_blank">demo video</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>        
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <a href="http://arxiv.org/pdf/1604.01431.pdf"><img border=0 src="img/FPIOC.gif" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>Forecasting Interactive Dynamics of Pedestrians with Fictitious Play</strong>
      <br>
      Wei-Chiu Ma, 
      <span class="collaborator">De-An Huang, </span>
      <span class="collaborator">Namhoon Lee, </span>
      <span class="collaborator">Kris M. Kitani </span>
      <br>
      <a href="" target="_new"><b>CVPR 2017</b></a> / 
      <a href="http://arxiv.org/pdf/1604.01431.pdf" target="_new">arXiv</a>
      </div>
    </td>
  </tr>
  
  <tr>
    <td>
      <img border=0 src="img/CVPR2015.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <p><strong>How Do We Use Our Hands? Discovering a Diverse Set of Common Grasps</strong>
      <br>
      <span class="collaborator">De-An Huang, </span>
      Wei-Chiu. Ma*, 
      <span class="collaborator">Minghuan Ma*, </span>
      <span class="collaborator">K. M. Kitani </span>
      <br>
      <a href="" target="_new"><b>CVPR 2015</b></a> / 
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yv3sH74AAAAJ&cstart=20&sortby=pubdate&citation_for_view=yv3sH74AAAAJ:HeT0ZceujKMC" target="_new">paper</a>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <img border=0 src="img/Hand-Object-Interaction.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <p><strong>Recognizing Hand-Object Interactions in Wearable Camera Videos</strong>
      <br>
      <span class="collaborator">Tatsuya Ishihara, </span>
      <span class="collaborator">Kris M. Kitani, </span>
      Wei-Chiu Ma, 
      <span class="collaborator">Hironobu Takagi, </span>
      <span class="collaborator">Chieko Asakawa </span>
      <br>
      <a href="" target="_new"><b>ICIP 2015</b></a> / 
      <a href="http://www.cs.cmu.edu/~kkitani/pdf/IKMTA-ICIP2015.pdf" target="_new">paper</a>
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <img border=0 src="img/TrafficGA.png" class="publogo">
    </td>
    <td>
      <div class="publication">
      <p><strong>Novel traffic signal timing adjustment strategy based on Genetic Algorithm</strong>
      <br>
      <span class="collaborator">Hsiao-Yu Tung*, </span>
      Wei-Chiu Ma*, 
      <span class="collaborator">Tian-Li Yu </span>
      <br>
      <a href="" target="_new"><b>CEC 2014</b></a> / 
      <a href="http://www.andrew.cmu.edu/user/weichium/publications/TrafficGA.pdf" target="_new">paper</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span>           
      </div>
    </td>
  </tr>

  <tr>
    <td>
      <a href="https://www.youtube.com/embed/TUwk1DSgl1Q"><img border=0 src="img/TDTOS-concept.png" class="publogo"></a>
    </td>
    <td>
      <div class="publication">
      <p><strong>TDTOS: T-Shirt Design and Try On System</strong>
      <br>
      <span class="collaborator">Chen-Yu Hsu*, </span>
      <span class="collaborator">Chi-Hsien Yen*, </span> 
      Wei-Chiu Ma*, 
      <span class="collaborator">Shao-Yi Chien </span>
      <br>
      <a href="" target="_new"><b>Asia-Pacific Workshop on FPGA Applications 2012</b></a> /       
      <!-- <br><b>Oral Presentation</b> (accpetance rate: 4.8%) -->
      <a href="https://www.youtube.com/embed/TUwk1DSgl1Q" target="_new">demo video</a>
      <br>
      <span class="highlight"><b>Oral presentation</b></span> 
      <br>
      <span class="highlight"><b>Best Application Award</b></span> 
      </div>
    </td>
  </tr>    
  </script>
  </table>
</div>
  </td>
  </tr>
  
  <tr>
 <td colspan = 7>
    <p>&nbsp;</p> 
  <h2 id="pro-bono">Pro bono office hour</h2>
      <p>Inspired by <a href="https://kyunghyuncho.me/office-hour-request/">Prof. Kyunghyun Cho</a> and <a href="https://krrish94.github.io/">Krishna Murthy</a>, starting January 2021, I have decided to commit 1~2 hours every week to provide guidance, suggestions, and/or mentorships for students from underrepresented groups or whoever is in need. Please fill in this <a href="https://docs.google.com/forms/d/1uYA95BMyFjnvY2-XDbwSAlTz79Ip55cytzrNoROS9ko/edit">form</a> if you are interested.</p>

      Need more (diverse) opinions? Consider talking to people with different expertise or from different background: <a href="https://www.tongzhouwang.info">Tongzhou Wang</a>,
      <a href="https://zhijianliu.com">Zhijian Liu</a>,
      <a href="https://www.zyrianov.org">Vlas Zyrianov</a>, 
      <a href="https://yshen47.github.io">Yuan Shen</a>, 
      <a href="https://www.cs.toronto.edu/~jungao/#pro-bono">Jun Gao</a>.
    </td>
 </tr>

  <tr>
  <td colspan = 7>
    <h2 id="misc">Misc.</h2>
    <ul>
        <li> I enjoy playing/watching all kinds of sports, including but not limited to badminton, basketball, baseball, tennis, running, etc. My favorite sports players and my favorite games (click their names!) are: 
          <ul>
          <li><a href="https://www.youtube.com/watch?v=GTJwoWHMEw0">Kobe Bryant</a> (basketball) </li>
          <li> <a href="https://www.youtube.com/watch?v=rctOtFOXco8">Roger Federer</a> (tennis)</li>
          <li> <a href="https://www.youtube.com/watch?v=Hf-n1yfd8II">Lee Chong Wei</a> (badminton)</li>
          </ul>
        </li>
        <br>
        <li> I like chinese pop music and my favorite singer is <a href="https://en.wikipedia.org/wiki/Jay_Chou">Jay Chou</a>. <!-- Here are a few personal favorite songs: -->      
        </li>
        <br>
        <li> A few personal favorite papers/books/courses can be found <a href="misc/favorite-books-papers.html">here</a> (unordered and unfinished).</li>
    </ul>
    </td>
    </tr>   
  <!--
  <tr>
  <td colspan = 7>
  <p>&nbsp;</p>
    <h2>Collaborators</h2>
    <ul>
        <li>Over the past few years, I have had the fortune to work with a bunch of great people: <a href="http://www.cs.toronto.edu/~slwang/">Shenlong Wang</a> (UofT), <a href="http://www.cs.toronto.edu/~mbrubake/">Marcus A. Brubaker</a> (UofT)</li>
    </ul>
    </td>
    </tr> -->
  

    </table>
    </center>
    </div>
    </div>
    <br><center><a href="http://accessibility.mit.edu/">Accessibility</a></center>
    </center>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11104199; 
var sc_invisible=1; 
var sc_security="50dd2089"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="free hit
counter" href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11104199/0/50dd2089/1/" alt="free
hit counter"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

<script>showPubs(1);</script>
</body>
</html>
