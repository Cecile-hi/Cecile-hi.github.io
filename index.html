
<!doctype html>
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
    <title>Xiaohui Zhang, BJTU</title>
    <style>
    h1 { padding : 0; margin : 0; }
    body { padding : 20px 0; font-family : Arial; font-size : 16px; background-image : url("img/bg.png"); } 
    #container { width : 900px; margin : 0 auto; background-color : #fff; padding : 50px;  text-align: left; box-shadow: 0px 0px 10px #999;; }
    #me { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0; margin-right:25px;}
    #content { display : block; margin-right : 275px;}
    a { text-decoration : none; }
    a:hover { text-decoration : underline; }
    a:link,a:visited
    {color: #1367a7;}    

    a.invisible { color : inherit; text-decoration : inherit; }
    .publogo { margin-top : 20px; margin-right : 10px; float : left; border : 0; width: 200px; vertical-align: middle;}
    
    .publication { clear : left; padding-bottom : 10px; line-height:22px;}
    .codelogo { margin-right : 10px; float : left; border : 0;}
    .code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
    .code .download a { display : block; margin : 0 15px; float : left;}
    #simpsons { margin : 5px auto; text-align : center; color : #B7B7B7; }

    span.highlight,
    span.highlight a:link,
    span.highlight a:visited{color:#BB2222}

    span.collaborator,
    span.collaborator a:link,
    span.collaborator a:visited{color:#666666}
    </style>

    <script type="text/javascript">
      function showPubs(id) {
        if (id == 0) {
          document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
          document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
          document.getElementById('select1').style = '';
          // document.getElementById('select2').style = '';
        } else if (id == 1) {
          document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_date').innerHTML;
          document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
          document.getElementById('select0').style = '';
        }
      } 
    </script>

</head>
<body>
  <!--#F2F2F2-->
    <center>
    <div id="container">
    <!-- <img src="img/Wei-Chiu-Ma.jpg" align = "right" width = 200> -->
    <img src="img/Personal_Page.png" align = "right" width = 250>
    <div id="content">
    <p><a href="index.html">Home</a></p>
    <p><a href="publications.html">Publications</a></p>
    <h1>Xiaohui Zhang 张晓辉</h1>
    <p>
      Interning @Institute of Automation, Chinese Academy of Science, Beijing, China<br>
      Master Student @ CS/Beijing Jiaotong University, Beijing, China<br>
      Bachelor @ ECE/Beijing Jiaotong University, Beijing, China<br>
      <br>
       <a href="mailto: 21120320@bjtu.edu.cn">Email</a> / <a href="https://scholar.google.com/citations?user=-7ZwZbMAAAAJ&hl=en">Google scholar</a></p>

    <h2>About Me</h2>   
    <p>I am a final year master student at <a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University (BJTU)</a>. Currently, I interning at <a href="http://www.ia.cas.cn/">Institute of Automation, Chinese Academy of Science(CASIA)</a> where I am advised by <a href="https://www.au.tsinghua.edu.cn/info/1104/2986.html">Jianhua Tao</a>. My research interests mainly lie in the intersection of machine learning and multi-modal learning, with particular interests in continual learning and signal processing tasks such as Deepfake audio detection and multimodal emotion recognition.</p>
    <p>
    </p>
    <p>
    My research lies at the intersection of continual learning and multi-modal learning. Specifically, I am interested in developing continuous multimodal tools that allow us to <b>model</b>, <b>reconstruct</b>, and <b>understand</b> the dynamic world from sparse, noisy, and unconstrained sensory data.
    <p><span class="highlight"><b>Prospective Advisers:</b></span> I am now looking for a full-time Ph.D. position in the fields of machine learning and multimodal learning. If you have any opportunities or recommendations in these areas, I would greatly appreciate your assistance. Please drop me an <a href="mailto: 21120320@bjtu.edu.cn"> email</a> directly :)
    </p>
  <center>
    <table border="0" cellpadding="0" cellspacing="0" width="900" align="center" bgcolor="#FFFFFF">   
      <tr>
        <td width="20"></td>
            <td width="120" align="center" valign="middle"><img src="img/bjtu.png" height="80" /></td>
            <td width="120" align="center" valign="middle"><img src="img/tsinghua.png" height="80" /></td>            
            <td width="120" align="center" valign="middle"><img src="img/casia.png" height="90" /></td>
        <td width="20"></td>
      </tr>
  <tr>
  <td colspan = 7>
  <p>&nbsp;</p>
    <h2>Recent News</h2>
    <ul>
<!--         <li><sup><font color="red"><b>NEW</b></font></sup> <b>Seminar</b>: I co-organize the <a href="https://sites.google.com/view/visionseminar">MIT Vision and Graphics Seminar.</a> Please reach out if you are interested in presenting.</li> -->         
        <li><sup><font color="red"><b>NEW</b></font></sup> <b>March. 2024</b>: I interned remotely at the <a href="https://www.unc.edu/"> University of North Carolina at Chapel Hill (UNC)</a> USA, under Prof. <a href='https://www.huaxiuyao.io/'>Huaxiu Yao</a> and Prof. <a href='https://cs.unc.edu/person/mohit-bansal/'>Mohit Bansal</a> about multimodal learning, our work had been accepted by <a href='https://cvpr.thecvf.com/'>CVPR 2024 </a>. Paper has already been released in <a href='https://arxiv.org/pdf/2311.10707.pdf'>[paper]</a>. Check it out!</li>
        <li><sup><font color="red"><b>NEW</b></font></sup> <b>Dec. 10th. 2023</b>: We are pleased to announce that our recent work on continual learning & Deepfake audio detection has been accepted by the 38th Annual AAAI Conference on Artificial Intelligence conference <a href='https://aaai.org/aaai-conference/'> (AAAI 2024) </a>. The paper and code has already been released in <a href='https://arxiv.org/abs/2312.09651'>[paper]</a> and <a href='https://github.com/Cecile-hi/Radian-Weight-Modification'>[github]</a>. Check it out!</li>
        <li><sup><font color="red"><b>NEW</b></font></sup> <b>April. 2023</b>: Our work on continual learning & Deepfake audio detection has been accepted by the Fortieth International Conference on Machine Learning <a href='https://icml.cc/'> (ICML 2023) </a>. The paper and code has already been released in <a href='https://arxiv.org/abs/2308.03300'>[paper]</a> and <a href='https://github.com/Cecile-hi/Regularized-Adaptive-Weight-Modification'>[github]</a>. Check it out!</li>
        <br>
        <li><b>Dec. 15th. 2023</b>: Our recent work about using multi-view entropy for robust deepfake audio detection has been accepted by the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing <a href='https://2024.ieeeicassp.org/'> (ICASSP 2024) </a>. The paper and code will be released in recent days. </li>                  
        <!-- <li><sup><font color="red"><b>NEW</b></font></sup> <b>Mar. 2023</b>: Check out our latest effort on closed-loop sensor simulation, LiDAR generation, and thermal imaging!</li>                -->
        <li><b>June. 2023</b>: Three papers accepted to the 32nd International Joint Conference on Artificial Intelligence <a href='https://ijcai-23.org/'> (IJCAI 2023 DADA Workshop) </a>. Check out our work on Deepfake Audio Detection & LoRA <a href='https://arxiv.org/pdf/2306.05617.pdf'>[papers]</a></li>      
        <li> <b>August. 2023</b>: One paper accepted to the 3th CAAI International Conference on Artificial Intelligence <a href='https://cicai.caai.cn/#/'> (CICAI 2023) </a>. Check out our work on Time-Sparse Transducer for Speech Recognition <a href='https://arxiv.org/pdf/2307.08323.pdf'>[paper]</a></li>      
        <li><b>June. 2023</b>: One paper on Deepfake Audio Detection has been submitted to the IEEE Transactions on Pattern Analysis and Machine Intelligence <a href='https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34'> (TPAMI) </a>.</li>
        <li><b>June. 2021</b>: Our work (A multilingual framework based on pre-training model for speech emotion recognition) is accepted to the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference 2021 <a href='http://www.apsipa.org/proceedings/2021/HTML/index.html'> (APSIPA 2021) </a>! Check it out! <a href='http://www.apsipa.org/proceedings/2021/pdfs/0000750.pdf'>[paper]</a></li>
       <li><b>June. 2021</b>: I graduated from the EECS department of <a href='https://www.bjtu.edu.cn/'> BJTU </a> --- thanks Mangui Liang for all your support in the past four years!</li>        
               
    </ul>
    </td>
    </tr>
<td colspan = 7>
  <h2 id="misc">Misc.</h2>
  <ul>
      <li> I enjoy playing/watching all kinds of sports, including but not limited to basketball, football, tennis, running, etc. My favorite sports players and my favorite games (click their names!) are: 
        <ul>
        <li><a href="https://youtu.be/1Dv63K05MzQ?si=ESpGvLR-XxwF53wk">Kyrie Irving</a> (basketball) </li>
        <li> <a href="https://www.youtube.com/watch?v=rctOtFOXco8">Roger Federer</a> (tennis)</li>
        <li> <a href="https://youtu.be/FuiJHJz4f5Q?si=FX8sncEXlWW_VHHl">Usain Bolt</a> (running)</li>
        </ul>
      </li>
      <br>
      <li> I like chinese GuFeng music and french folk, my favorite singer is <a href="https://en.wikipedia.org/wiki/C%C3%A9cile_Corbel">Cécile Corbel</a>. <!-- Here are a few personal favorite songs: -->      
      </li>
      <br>
      <li> A few personal favorite papers/books/courses can be found <a href="misc/favorite-books-papers.html">here</a> (unordered and unfinished).</li>
  </ul>
  </td>
  </tr>   
<!--
<tr>
<!-- End of StatCounter Code for Default Guide -->

<script>showPubs(1);</script>
</body>
</html>
